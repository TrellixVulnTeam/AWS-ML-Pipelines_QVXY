{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Creation using SageMaker Pipelines - Complete\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "1. Define a set of Pipeline parameters that can be used to parametrize a SageMaker Pipeline.\n",
    "2. Define a Processing step that performs cleaning, feature engineering, and splitting the input data into train and test data sets.\n",
    "3. Define a Training step that trains a model on the preprocessed train data set.\n",
    "4. Define a Processing step that evaluates the trained model's performance on the test dataset.\n",
    "5. Define a Create Model step that creates a model from the model artifacts used in training.\n",
    "6. Define a Transform step that performs batch transformation based on the model that was created.\n",
    "7. Define a Register Model step that creates a model package from the estimator and model artifacts used to train the model.\n",
    "8. Define a Conditional step that measures a condition based on output from prior steps and conditionally executes other steps.\n",
    "9. Define and create a Pipeline definition in a DAG, with the defined parameters and steps.\n",
    "10. Start a Pipeline execution and wait for execution to complete.\n",
    "11. Download the model evaluation report from the S3 bucket for examination.\n",
    "12. View the lineage of the artifacts of the Pipeline.\n",
    "13. Start a second Pipeline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A typical ML Application pipeline](./img/pipeline-full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger,ParameterString\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f'AbaloneML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Default bucket = {bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prep data\n",
    "\n",
    "The dataset you use is the [UCI Machine Learning Abalone Dataset](https://archive.ics.uci.edu/ml/datasets/abalone).  The aim for this task is to determine the age of an abalone from its physical measurements. At the core, this is a regression problem.\n",
    "\n",
    "Predict age based on physical measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/abalone.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./data/abalone.csv s3://{bucket}/abalone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./data/abalone-unlabeled s3://{bucket}/abalone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = f's3://{bucket}/abalone/abalone.csv'\n",
    "batch_data_uri = f's3://{bucket}/abalone/abalone-unlabeled' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Pipeline-level parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name='ProcessingInstanceCount', default_value=1)\n",
    "processing_instance_type = ParameterString(name='ProcessingInstanceType', default_value='ml.m5.xlarge')\n",
    "training_instance_type = ParameterString(name='TrainingInstanceType', default_value='ml.m5.xlarge')\n",
    "model_approval_status = ParameterString(name='ModelApprovalStatus', default_value='Approved')\n",
    "input_data = ParameterString(name='InputData', default_value=input_data_uri)\n",
    "batch_data = ParameterString(name='BatchData', default_value=batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define Parameters](./img/pipeline-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill in missing sex category data and encode it so that it is suitable for training.\n",
    "* Scale and normalize all numerical fields, aside from sex and rings numerical data.\n",
    "* Split the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/preprocessing.py\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import requests\n",
    "import tempfile\n",
    "import logging\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "logger.info(f'Using Sklearn version: {sklearn.__version__}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('Sklearn Preprocessing Job [Start]')\n",
    "    base_dir = '/opt/ml/processing'\n",
    "\n",
    "    df = pd.read_csv(f'{base_dir}/input/abalone.csv')\n",
    "    y = df.pop('rings')\n",
    "    cols = df.columns\n",
    "    logger.info(f'Columns = {cols}')\n",
    "\n",
    "    numeric_features = list(df.columns)\n",
    "    numeric_features.remove('sex')\n",
    "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), \n",
    "                                          ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_features = ['sex']\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                              ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocess = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), \n",
    "                                                 ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f'{base_dir}/train/train.csv', header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f'{base_dir}/validation/validation.csv', header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(f'{base_dir}/test/test.csv', header=False, index=False)\n",
    "    logger.info('Sklearn Preprocessing Job [End]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_version = '0.23-1'\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version=framework_version, \n",
    "                                     instance_type=processing_instance_type, \n",
    "                                     instance_count=processing_instance_count, \n",
    "                                     base_job_name='sklearn-abalone-preprocess', \n",
    "                                     role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_process = ProcessingStep(name='AbalonePreprocess', \n",
    "                              processor=sklearn_processor, \n",
    "                              inputs=[ProcessingInput(source=input_data, destination='/opt/ml/processing/input')], \n",
    "                              outputs=[ProcessingOutput(output_name='train', source='/opt/ml/processing/train'), \n",
    "                                       ProcessingOutput(output_name='validation', source='/opt/ml/processing/validation'), \n",
    "                                       ProcessingOutput(output_name='test', source='/opt/ml/processing/test')], \n",
    "                              code='src/preprocessing.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f's3://{bucket}/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(framework='xgboost', \n",
    "                                          region=region, \n",
    "                                          version='1.0-1', \n",
    "                                          py_version='py3', \n",
    "                                          instance_type=training_instance_type)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = Estimator(image_uri=image_uri, \n",
    "                      instance_type=training_instance_type, \n",
    "                      instance_count=1, \n",
    "                      output_path=model_path, \n",
    "                      role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train.set_hyperparameters(objective='reg:squarederror', \n",
    "                              num_round=50, \n",
    "                              max_depth=5, \n",
    "                              eta=0.2, \n",
    "                              gamma=4, \n",
    "                              min_child_weight=6, \n",
    "                              subsample=0.7, \n",
    "                              silent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(name='AbaloneTraining', \n",
    "                          estimator=xgb_train, \n",
    "                          inputs={'train': TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri, \n",
    "                                                         content_type='text/csv'), \n",
    "                                  'validation': TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri, \n",
    "                                                              content_type='text/csv')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Training Step to Train a Model](img/pipeline-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, develop an evaluation script that is specified in a Processing step that performs the model evaluation.\n",
    "\n",
    "After pipeline execution, you can examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script uses `xgboost` to do the following:\n",
    "\n",
    "* Load the model.\n",
    "* Read the test data.\n",
    "* Issue predictions against the test data.\n",
    "* Build a classification report, including accuracy and ROC curve.\n",
    "* Save the evaluation report to the evaluation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/evaluation.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import pathlib\n",
    "import xgboost\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    model_path = '/opt/ml/processing/model/model.tar.gz'\n",
    "    \n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "\n",
    "    model = pickle.load(open('xgboost-model', 'rb'))\n",
    "\n",
    "    test_path = '/opt/ml/processing/test/test.csv'\n",
    "    \n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {'regression_metrics': \n",
    "                   {'mse': \n",
    "                    {'value': mse, 'standard_deviation': std}\n",
    "                   }\n",
    "                  }\n",
    "\n",
    "    output_dir = '/opt/ml/processing/evaluation'\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f'{output_dir}/evaluation.json'\n",
    "    with open(evaluation_path, 'w') as f: \n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "# using the same XGBoost training image from the previous step\n",
    "script_eval = ScriptProcessor(image_uri=image_uri, \n",
    "                              command=['python3'], \n",
    "                              instance_type=processing_instance_type, \n",
    "                              instance_count=1, \n",
    "                              base_job_name='AbaloneEvaluate', \n",
    "                              role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report = PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation.json')\n",
    "\n",
    "step_eval = ProcessingStep(name='AbaloneEvaluate', \n",
    "                           processor=script_eval, \n",
    "                           inputs=[ProcessingInput(source=step_train.properties.ModelArtifacts.S3ModelArtifacts, \n",
    "                                                   destination='/opt/ml/processing/model'), \n",
    "                                   ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri, \n",
    "                                                   destination='/opt/ml/processing/test')], \n",
    "                           outputs=[ProcessingOutput(output_name='evaluation', source='/opt/ml/processing/evaluation')], \n",
    "                           code='src/evaluation.py', \n",
    "                           property_files=[evaluation_report])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same XGBoost container image used for training and evaluation in the previous steps\n",
    "model = Model(image_uri=image_uri, \n",
    "              model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts, \n",
    "              sagemaker_session=sagemaker_session, \n",
    "              role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = CreateModelInput(instance_type='ml.m5.large')\n",
    "\n",
    "step_create_model = CreateModelStep(name='AbaloneCreateModel', \n",
    "                                    model=model, \n",
    "                                    inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Batch Transform Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(model_name=step_create_model.properties.ModelName, \n",
    "                          instance_type='ml.m5.xlarge', \n",
    "                          instance_count=1, \n",
    "                          output_path=f's3://{bucket}/AbaloneTransform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_transform = TransformStep(name='AbaloneTransform', \n",
    "                               transformer=transformer, \n",
    "                               inputs=TransformInput(data=batch_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Register Model Step to Create a Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(model_statistics=MetricsSource(\n",
    "    s3_uri='{}/evaluation.json'.format(step_eval.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']),\n",
    "    content_type='application/json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register = RegisterModel(name='AbaloneRegisterModel', \n",
    "                              estimator=xgb_train, \n",
    "                              model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,  \n",
    "                              content_types=['text/csv'], \n",
    "                              response_types=['text/csv'], \n",
    "                              inference_instances=['ml.t2.medium', 'ml.m5.xlarge'], \n",
    "                              transform_instances=['ml.m5.xlarge'], \n",
    "                              model_package_group_name=model_package_group_name, \n",
    "                              approval_status=model_approval_status, \n",
    "                              model_metrics=model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Create Model Step and Batch Transform to Process Data in Batch at Scale](img/pipeline-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conditional Step\n",
    "\n",
    "##### Define a Condition Step to Check Accuracy and Conditionally Create a Model and Run a Batch Transformation and Register a Model in the Model Registry\n",
    "\n",
    "In this step, the model is registered only if the accuracy of the model, as determined by the evaluation step `step_eval`, exceeded a specified value. A `ConditionStep` enables pipelines to support conditional execution in the pipeline DAG based on the conditions of the step properties. \n",
    "\n",
    "In the following section, you:\n",
    "\n",
    "* Define a `ConditionLessThanOrEqualTo` on the accuracy value found in the output of the evaluation step, `step_eval`.\n",
    "* Use the condition in the list of conditions in a `ConditionStep`.\n",
    "* Pass the `CreateModelStep` and `TransformStep` steps, and the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`, which are only executed, if the condition evaluates to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_lte = ConditionLessThanOrEqualTo(left=JsonGet(step=step_eval, \n",
    "                                                   property_file=evaluation_report, \n",
    "                                                   json_path='regression_metrics.mse.value'), \n",
    "                                      right=6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(name='AbaloneMSECond', \n",
    "                          conditions=[cond_lte], \n",
    "                          if_steps=[step_register, step_create_model, step_transform], \n",
    "                          else_steps=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Condition Step to Check Accuracy and Conditionally Execute Steps](img/pipeline-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Define and create a Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'AbalonePipeline'\n",
    "\n",
    "pipeline = Pipeline(name=pipeline_name, \n",
    "                    parameters=[processing_instance_type, \n",
    "                                processing_instance_count, \n",
    "                                training_instance_type,\n",
    "                                model_approval_status,\n",
    "                                input_data, \n",
    "                                batch_data], \n",
    "                    steps=[step_process, step_train, step_eval, step_cond])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Pipeline of Parameters, Steps, and Conditions](img/pipeline-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Kickstart Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Examine Evaluation Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    '{}/evaluation.json'.format(step_eval.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']))\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. View Lineage of the artifacts of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = LineageTableVisualizer(sagemaker.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(visualizer.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Start a second Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run additional executions of the pipeline and specify different pipeline parameters. The parameters argument is a dictionary containing parameter names, and where the values are used to override the defaults values.\n",
    "\n",
    "Based on the performance of the model, you might want to kick off another pipeline execution on a compute-optimized instance type and set the model approval status to \"Approved\" automatically. This means that the model package version generated by the `RegisterModel` step is automatically ready for deployment through CI/CD pipelines, such as with SageMaker Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(parameters=dict(ProcessingInstanceType='ml.c5.xlarge', \n",
    "                                           ModelApprovalStatus='PendingManualApproval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
